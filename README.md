# Kafka-Redshift-Pyspark-Project

In this project, i used Apache kafka, AWS Redshift data warehouse and pyspark to build Data pipeline.In the beginning i used pandas to transform the extracted csv dataframe before finally using pyspark to create the ETL for this project.I showcased how to create topic on confluent-kafka and load the data to it then consume the same data using kafta consumer.Plus, i integrate pyspark with AWS Redshift cluster to load the transformed to its created database and table gaining the privilege to query the data on the Redshift data warehouse.

# Dataset tools used for this project

- Confluenet Kafka
- Pyspark
- Pandas
- AWS Redshift Datawarehouse
- Jupyter notebook
